@article{Nuzzo:2014bp,
author = {Nuzzo, Regina},
title = {{Statistical errors}},
journal = {Nature},
year = {2014},
volume = {506},
number = {7487},
pages = {150--152},
annote = {{\#} rather than being convenient short- hand for significance, the P value is a specific measure developed to test whether results touted as evidence for an effect are likely to be observed if the effect is not real. It says nothing about the likelihood of the effect in the first place.

{\#} P values were not meant to be a definitive test

{\#} P value of 0.01: According to one widely used calculation, a P value of 0.01 corresponds to a false-alarm probability of at least 11%, depending on the underlying probability that there is a true effect; a P value of 0.05 raises that chance to at least 29%. 

{\#} significance is no indicator of practical relevance: We should be asking {\textquoteleft}How much of an effect is there?{\textquoteright}, not {\textquoteleft}Is there an effect?{\textquoteright}{\textquotedblright}

{\#} To pounce on tiny P values and ignore the larger question is to fall prey to the {\textquotedblleft}seductive certainty of significance{\textquotedblright}

{\#} To avoid the trap of thinking about results as significant or not significant, for example, Cumming thinks that researchers should ALWAYS REPORT EFFECT SIZES AND CONFIDENCE INTERVALS. These convey what a P value does not: the magnitude and relative importance of an effect.

{\#} We report how we determined our sample size, all data exclusions (if any), all manipulations and all measures in the study.


}
}